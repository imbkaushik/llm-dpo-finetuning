{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c973c5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q pandas torch transformers accelerate\n",
    "\n",
    "# Import libraries and suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import random\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from google.colab import files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50780e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to Hugging Face to access your fine-tuned model\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073bcaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "HF_USERNAME = \"your_user_name\"\n",
    "FT_MODEL_NAME = \"Qwen2.5-0.5B-DPO-YouTube-Titles\"\n",
    "\n",
    "BASE_MODEL_NAME = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "YOUR_FT_MODEL_ID = f\"{HF_USERNAME}/{FT_MODEL_NAME}\"\n",
    "NUM_IDEAS_TO_TEST = 10 # Number of random ideas to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b482c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_ideas = pd.read_csv('ideas.csv', header=None)\n",
    "    idea_list = df_ideas[0].tolist()\n",
    "\n",
    "    if idea_list and idea_list[0].strip().lower() == 'idea':\n",
    "        idea_list.pop(0)\n",
    "\n",
    "    # Select a random sample of ideas for testing\n",
    "    if len(idea_list) < NUM_IDEAS_TO_TEST:\n",
    "        test_ideas = idea_list\n",
    "    else:\n",
    "        random.seed(42)\n",
    "        test_ideas = random.sample(idea_list, NUM_IDEAS_TO_TEST)\n",
    "    print(f\"Loaded and sampled {len(test_ideas)} ideas for evaluation.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: 'ideas.csv' not found. Please make sure you have uploaded it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d848ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME)\n",
    "base_tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "\n",
    "print(\"Loading fine-tuned model from Hugging Face Hub...\")\n",
    "try:\n",
    "    ft_model = AutoModelForCausalLM.from_pretrained(YOUR_FT_MODEL_ID)\n",
    "    ft_tokenizer = AutoTokenizer.from_pretrained(YOUR_FT_MODEL_ID)\n",
    "    print(\"Models loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading fine-tuned model '{YOUR_FT_MODEL_ID}'.\")\n",
    "    print(f\"Error details: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d12ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def generate_response(prompt, model, tokenizer, max_length=100, temperature=0.7, device='mps'):\n",
    "    \"\"\"\n",
    "    Generate a response for a given prompt using a Hugging Face text generation pipeline.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): Input prompt string.\n",
    "        model: Fine-tuned language model.\n",
    "        tokenizer: Corresponding tokenizer.\n",
    "        max_length (int): Maximum length of the generated response.\n",
    "        temperature (float): Sampling temperature for generation.\n",
    "        device (str): Device to run the model on. For example, 'cpu', 'cuda', or 'mps'.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated text output.\n",
    "    \"\"\"\n",
    "\n",
    "    generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=device)\n",
    "    outputs = generator(prompt, max_length=max_length, truncation=True, num_return_sequences=1, temperature=temperature)\n",
    "    return outputs[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16dcf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Run Evaluation ---\n",
    "results = []\n",
    "print(\"\\nGenerating titles for comparison...\")\n",
    "for i, idea in enumerate(test_ideas):\n",
    "    print(f\"  - Processing idea {i+1}/{len(test_ideas)}: '{idea}'\")\n",
    "\n",
    "    base_title = generate_title(idea, base_model, base_tokenizer)\n",
    "    ft_title = generate_title(idea, ft_model, ft_tokenizer)\n",
    "\n",
    "    results.append({\n",
    "        \"idea\": idea,\n",
    "        \"base_model_title\": base_title,\n",
    "        \"fine_tuned_title\": ft_title\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cdadf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Display and Download Results ---\n",
    "print(\"\\n--- Evaluation Results ---\")\n",
    "print(df_results)\n",
    "\n",
    "# Save results to a CSV file\n",
    "output_filename = \"evaluation_results.csv\"\n",
    "df_results.to_csv(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ce4632",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"evaluation_results.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1c23477",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcfbcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the output file\n",
    "files.download(output_filename)\n",
    "print(f\"\\nResults saved and download prompted for '{output_filename}'.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
